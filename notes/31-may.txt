* Policzyć liczbę słów w bazie danych
* Version of the stanford parser which gives grammatical functions (subject, object)
* Signals -- only tokens (i.e., POS tag nodes)

=== AFTER-THOUGHTS ===

# Grammatical functions

* The stanford parser does not provide grammatical functions (subject, object, etc.).
  It seems that there is no option to obtain them using the standard stanford model.

# Tokenization

* It is possible to enforce a custom tokenization, see

    https://nlp.stanford.edu/software/parser-faq.shtml#q

* It is even possible to enforce some tags, see

    https://nlp.stanford.edu/software/parser-faq.shtml#f

  Note that "partially-tagged input (only indicating the POS of some words) is also OK."

* If we decide to use the standard tokenization/POS-tagging format, i.e. like this:

    The/DT quick/JJ brown/JJ fox/NN jumped/VBD over/IN the/DT lazy/JJ dog/NN ./. 

  (I) We might proceed as follows:  

  1. First we process the source Ancor XML files in order to retrieve tokenized output.
     This is a stand-alone task.  We just need to chose a reasonable format and keep
     the character numberings.  The nice thing is that, afterwards, we can forget about
     the different XML format, different speech-related annotation details, etc., and
     just work with the tokenized text.
     
     Let's thus say that we have a file called "fileID.tok".

  2. Then, we need to simplify "fileID.tok" in order to get the input consistent with the
     Stanford POS-tagging/tokenization format.  In particular, we need to add programatical
     support for this format.

  3. PROBLEM! On the one hand, we want to tokenize the input ourselves, because we want to
     keep track of the references to the original source Ancor files.  On the other hand, we
     want to use Stanford to do the tokenization for us, so that it is consistent with the
     parsing model.  To do both seems impossible?

     The idea raised during the meeting was to send to (i) somehow do the tokenization, 
     in a way which preserves the addresses referring to the sources files, and
     (ii) send the tokens to parse.  The point is that, in step (i), we have to do some kind of
     alignement anyway, so I don't see what we gain by doint it in Java?

  (II) Let's try again:

  1. First we process the source Ancor XML files in order to chunk each raw
     sentence into the fragments of real text and speech-related annotations (e.g.
     [pi]) interleaved.  The raw fragments can be pre-processed (e.g., é(tais) ->
     étais).  It is important that each chunk provides the beg and the end markers
     which refer to the source files (character-based addressing).
     
     Let's thus say that we have a file called "fileID.chunks".

  2. We simplify "fileID.chunks" in order to obtain raw sentence to parse,
     one per line, in a file called "fileID.txt".

  3. We parse "fileID.txt" using stanford, we obtain "fileID.penn".  Now we have to
     project back each parse tree in "fileID.penn" on the corresponding sentence in
     "fildID.chunks".  As a result, we should obtain both tokenization and syntactic
     information.  Based on the beg/end addresses assigned to chunks, we can try
     to recalculate the addresses corresponding to tokens, and for each token, its
     original wordform (recall that the parsed text is pre-processed, e.g. "U B S"
     -> "UBS").

  (III) And if decide to use the parser's programmatical API?

  1. The first step is as above: we create the individual "fildID.chunks" files.

  2. Then, we need to implement in Java: (a) the XML parser for the chunks format, 
     (b) the custom tokenization "protocol", for starters.

  3. The custom tokenization: ...

  (IV) Let's imagine that we have a parsing HTTP server:

  1. We create the chunks, but don't have to store them on disk.
  2. For each chunk, we (i) create a simplified text to tokenize and parse,
     (ii) we parse the simplified part using stanford, and (iii) we perform
     the alignement. 

  To sum up, we have to do the alignement anyway, and I don't see how this would
  be easier in Java than it is in Haskell.
